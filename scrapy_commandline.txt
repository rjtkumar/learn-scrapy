To create a scrapy project:
    > scrapy startproject <project_name>
    > scrapy startproject chocolatescraper

To create a spider, go inside the project directory:
    > scrapy genspider <spider_name> <URL to be scraped>
    chocolatescraper> scrapy genspider chocolate "https://www.chocolate.co.uk/collections/all"

To list all spiders in a project, go inside the project directoryL
    chocolatescraper> scrapy list

To open the scrapy shsell:
    > scrapy shell

To run a spider:
    project_directory> scrapy crawl <spider_name>
    chocolatescraper> scrapy crawl chocolatespider
    To save teh scraped data using scrapy feed exporters:
        (json) chocolatescraper> scrapy crawl chocolatespider -o mydata.json
        (csv) chocolatescraper> scrapy crawl chocolatespider -o mydata.csv
        (json-lines) chocolatescraper> scrapy crawl chocolatespider -o mydata.jsonl
        (XML) chocolatescraper> scrapy crawl chocolatespider -o mydata.xml
    -o (lowercase) option to append to the end of the file if it already exists
    -O (uppercase) will delete any existing files of the same name and create new ones

Saving scraped data to an AWS bucket
    To be able to store to an aws bucket we need a python library called botocore
        > pip3 install botocore
    Once we havev teh library we can store to the bucket using the following command template
        chocolatescraper> scrapy crawl chocolatespider -O s3://aws_key:aws_secret@mybucketname/path/to/destination/scrapeddata.csv:csv

Library for fake user agents downloader middleware:
    > pip install scrapy-user-agents